{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62181a6f-3e8a-48a5-96ca-707e4d0a5c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“– Starting Kanda 1 having 77 Sargas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kanda 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                              | 45/77 [00:56<00:40,  1.26s/it]\n",
      "Kanda 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:49<00:00,  1.54sarga/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed Kanda 1 successfully!\n",
      "\n",
      "\n",
      "ðŸ“– Starting Kanda 2 having 119 Sargas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kanda 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [01:34<00:00,  1.26sarga/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed Kanda 2 successfully!\n",
      "\n",
      "\n",
      "ðŸ“– Starting Kanda 3 having 75 Sargas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kanda 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:49<00:00,  1.51sarga/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed Kanda 3 successfully!\n",
      "\n",
      "\n",
      "ðŸ“– Starting Kanda 4 having 67 Sargas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kanda 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:56<00:00,  1.19sarga/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed Kanda 4 successfully!\n",
      "\n",
      "\n",
      "ðŸ“– Starting Kanda 5 having 68 Sargas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kanda 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:45<00:00,  1.48sarga/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed Kanda 5 successfully!\n",
      "\n",
      "\n",
      "ðŸ“– Starting Kanda 6 having 131 Sargas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kanda 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131/131 [02:17<00:00,  1.05s/sarga]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed Kanda 6 successfully!\n",
      "\n",
      "\n",
      "ðŸŽ‰ Completed fetching Valmiki Ramayana!\n",
      "Data saved to: valmiki_ramayana1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import requests  # For making HTTP requests to fetch webpage data\n",
    "from bs4 import BeautifulSoup  # For parsing and extracting content from HTML\n",
    "from tqdm import tqdm  # For showing progress bars in loops\n",
    "\n",
    "# A dictionary containing the number of Sargas (chapters) in each Kanda (book) of the Valmiki Ramayana\n",
    "kanda_sarga_map = {\n",
    "    1: 77,   # Bala Kanda\n",
    "    2: 119,  # Ayodhya Kanda\n",
    "    3: 75,   # Aranya Kanda\n",
    "    4: 67,   # Kishkindha Kanda\n",
    "    5: 68,   # Sundara Kanda\n",
    "    6: 131   # Yuddha Kanda\n",
    "}\n",
    "\n",
    "# Base URL with placeholders to be formatted with Kanda and Sarga values\n",
    "base_url = \"https://www.valmiki.iitk.ac.in/sloka?field_kanda_tid={}&language=dv&field_sarga_value={}\"\n",
    "\n",
    "# Output file where the extracted content will be saved\n",
    "output_file = \"valmiki_ramayana1.txt\"\n",
    "\n",
    "# Opening the output file in write mode with UTF-8 encoding\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "\n",
    "    # Loop through each Kanda in the dictionary\n",
    "    for kanda, total_sargas in kanda_sarga_map.items():\n",
    "        print(f\"\\nðŸ“– Starting Kanda {kanda} having {total_sargas} Sargas\")\n",
    "\n",
    "        # Initialize a progress bar for the current Kanda\n",
    "        pbar = tqdm(total=total_sargas, desc=f\"Kanda {kanda}\", unit='sarga', position=0, leave=True)\n",
    "\n",
    "        # Loop through all Sargas in the current Kanda\n",
    "        for sarga in range(1, total_sargas + 1):\n",
    "            # Format the URL for the current Kanda and Sarga\n",
    "            url = base_url.format(kanda, sarga)\n",
    "            # Send a GET request to the webpage\n",
    "            response = requests.get(url)\n",
    "\n",
    "            # Initialize a list to store the extracted sloka lines\n",
    "            sarga_lines = []\n",
    "\n",
    "            # If the request was successful\n",
    "            if response.status_code == 200:\n",
    "                # Parse the HTML content using BeautifulSoup\n",
    "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                # Find all relevant rows containing sloka data\n",
    "                rows = soup.find_all(\"div\", class_=\"views-row\")\n",
    "\n",
    "                # Extract the Sanskrit sloka text from each row\n",
    "                for row in rows:\n",
    "                    text_div = row.find(\"div\", class_=\"views-field views-field-field-htetrans\")\n",
    "                    if text_div:\n",
    "                        text = text_div.get_text(strip=True)  # Remove leading/trailing spaces\n",
    "                        sarga_lines.append(text)\n",
    "\n",
    "                # Write the current Sarga to the file if it contains valid lines\n",
    "                if sarga_lines:\n",
    "                    file.write(f\"\\n\\n### Kanda {kanda} - Sarga {sarga} ###\\n\")\n",
    "                    file.write(\"\\n\".join(sarga_lines) + \"\\n\")\n",
    "\n",
    "            else:\n",
    "                # If the request failed, print an error message\n",
    "                print(f\"âŒ Failed to fetch Kanda {kanda} - Sarga {sarga}\")\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "        # Close the progress bar after the Kanda is done\n",
    "        pbar.close()\n",
    "        print(f\"âœ… Completed Kanda {kanda} successfully!\\n\")\n",
    "\n",
    "# Print completion message after all Kandams are scraped\n",
    "print(\"\\nðŸŽ‰ Completed fetching Valmiki Ramayana!\")\n",
    "print(f\"Data saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde59c94-af68-45c3-a7ac-b0568fc92915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV created successfully: clean_sanskrit_english.csv\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import re  # For using regular expressions to detect and extract structured patterns\n",
    "import pandas as pd  # For handling tabular data and exporting it to CSV\n",
    "\n",
    "# Initialize an empty list to store the final structured data\n",
    "data = []\n",
    "\n",
    "# Variables to hold the current Kanda and Sarga while parsing\n",
    "current_kanda = None\n",
    "current_sarga = None\n",
    "\n",
    "# Open the previously scraped text file for reading\n",
    "with open('valmiki_ramayana1.txt', 'r', encoding='utf-8') as file:\n",
    "    prev_english = \"\"  # To temporarily hold English text when Sanskrit is missing\n",
    "\n",
    "    # Read the file line by line\n",
    "    for line in file:\n",
    "        line = line.strip()  # Remove leading/trailing whitespace\n",
    "\n",
    "        # Detect the line indicating a new Kanda and Sarga using regex pattern\n",
    "        kanda_sarga_match = re.match(r'###\\s*Kanda\\s*(\\d+)\\s*-\\s*Sarga\\s*(\\d+)\\s*###', line)\n",
    "        if kanda_sarga_match:\n",
    "            # If matched, update the current Kanda and Sarga context\n",
    "            current_kanda = int(kanda_sarga_match.group(1))\n",
    "            current_sarga = int(kanda_sarga_match.group(2))\n",
    "            continue  # Skip processing this line further\n",
    "\n",
    "        if not line:\n",
    "            continue  # Skip empty lines\n",
    "\n",
    "        # Use regex to split Sanskrit and English parts:\n",
    "        # It groups Devanagari (Sanskrit) characters separately from English/alphanumeric characters\n",
    "        parts = re.findall(r'([\\u0900-\\u097F:|à¥¤à¥¥;()\\'\\-\\s]+|[a-zA-Z0-9 .;\\-:()\\']+)', line)\n",
    "\n",
    "        temp = []  # Temporary list to store pairs of [Sanskrit, English]\n",
    "\n",
    "        # Iterate over each detected part\n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            if not part:\n",
    "                continue\n",
    "\n",
    "            # If the part contains Devanagari characters, it's Sanskrit\n",
    "            if re.search('[\\u0900-\\u097F]', part):\n",
    "                part = part.lstrip(',')  # Clean any leading commas\n",
    "                temp.append([part, \"\"])  # Add Sanskrit word with placeholder for English\n",
    "            else:\n",
    "                # If it's English and there's already a Sanskrit word waiting\n",
    "                if temp:\n",
    "                    if temp[-1][1] == \"\":\n",
    "                        temp[-1][1] = part  # First English meaning for that Sanskrit word\n",
    "                    else:\n",
    "                        temp[-1][1] += \" \" + part  # Append additional meaning if it continues\n",
    "                else:\n",
    "                    # If no Sanskrit is present, store English in buffer for next valid pair\n",
    "                    prev_english += \" \" + part\n",
    "\n",
    "        # Add all collected [Sanskrit, English] word pairs to the main data list\n",
    "        for sanskrit, english in temp:\n",
    "            data.append([current_kanda, current_sarga, sanskrit, english.strip()])\n",
    "\n",
    "        # If the line contained only English and not Sanskrit,\n",
    "        # append this English text to the last entry's English column\n",
    "        if not temp and prev_english:\n",
    "            if data:\n",
    "                data[-1][3] += \" \" + prev_english.strip()\n",
    "            prev_english = \"\"\n",
    "\n",
    "# Convert the list of data into a pandas DataFrame for structured tabular storage\n",
    "df = pd.DataFrame(data, columns=[\"Kanda\", \"Sarga\", \"Sanskrit\", \"English\"])\n",
    "\n",
    "# Export the DataFrame to a CSV file with UTF-8 encoding\n",
    "df.to_csv('clean_sanskrit_english.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Final confirmation print\n",
    "print(\"CSV created successfully: clean_sanskrit_english.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9948c603-2e32-490f-bd6c-1acdbcce450b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File generated successfully: final_sanskrit_output.csv\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  # For working with tabular data\n",
    "import re  # For regular expressions used in phonetic formatting\n",
    "from indic_transliteration import sanscript  # For specifying script formats\n",
    "from indic_transliteration.sanscript import transliterate  # For transliteration between scripts\n",
    "\n",
    "# Load the cleaned Sanskrit-English word dataset\n",
    "df = pd.read_csv('clean_sanskrit_english.csv')\n",
    "\n",
    "# ----------------------------------------\n",
    "# Function: transliterate_to_iast\n",
    "# Purpose: Convert Devanagari script to IAST (International Alphabet of Sanskrit Transliteration)\n",
    "# Input: Sanskrit string in Devanagari script\n",
    "# Output: Corresponding IAST string\n",
    "# ----------------------------------------\n",
    "def transliterate_to_iast(text):\n",
    "    return transliterate(text, sanscript.DEVANAGARI, sanscript.IAST)\n",
    "\n",
    "# ----------------------------------------\n",
    "# Function: generate_phonetics\n",
    "# Purpose: Generate readable phonetic representation from IAST transliteration\n",
    "# Input: IAST string (from previous function)\n",
    "# Output: Pronunciation-friendly string with syllables hyphenated\n",
    "# ----------------------------------------\n",
    "def generate_phonetics(iast_text):\n",
    "    phonetic = iast_text.lower()  # Convert to lowercase for uniformity\n",
    "\n",
    "    # Replace IAST diacritics and complex sounds with English-friendly equivalents\n",
    "    replacements = [\n",
    "        ('kh', 'kh'), ('gh', 'gh'), ('ch', 'chh'), ('jh', 'jh'),\n",
    "        ('á¹­h', 'th'), ('á¸h', 'dh'), ('th', 'th'), ('dh', 'dh'),\n",
    "        ('ph', 'ph'), ('bh', 'bh'),\n",
    "        ('Ä', 'aa'), ('Ä«', 'ee'), ('Å«', 'oo'), ('á¹›', 'ri'), ('á¹', 'ree'),\n",
    "        ('ai', 'ai'), ('au', 'au'),\n",
    "        ('á¹…', 'ng'), ('Ã±', 'ny'), ('á¹­', 't'), ('á¸', 'd'), ('á¹‡', 'n'),\n",
    "        ('Å›', 'sh'), ('á¹£', 'sh'), ('á¸¥', 'h'), ('á¹ƒ', 'm')\n",
    "    ]\n",
    "\n",
    "    # Apply all replacements sequentially\n",
    "    for src, tgt in replacements:\n",
    "        phonetic = phonetic.replace(src, tgt)\n",
    "\n",
    "    # Add hyphens after each Sanskrit consonant for syllable separation\n",
    "    phonetic = re.sub(r'([kgcjá¹­á¸tdnpbmyrlvsh])', r'\\1-', phonetic)\n",
    "\n",
    "    # Remove trailing hyphen if added at the end\n",
    "    phonetic = phonetic.rstrip('-')\n",
    "\n",
    "    return phonetic\n",
    "\n",
    "# Initialize an empty list to store final output rows\n",
    "final_output = []\n",
    "\n",
    "# Loop over each row in the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    sanskrit = str(row['Sanskrit']).strip()  # Get Sanskrit word\n",
    "    english = str(row['English']).strip()  # Get English meaning\n",
    "\n",
    "    # Generate IAST transliteration from Sanskrit\n",
    "    translit = transliterate_to_iast(sanskrit)\n",
    "\n",
    "    # Generate phonetic breakdown from the IAST transliteration\n",
    "    phonetic = generate_phonetics(translit)\n",
    "\n",
    "    # Append the full row to the final output list\n",
    "    final_output.append([sanskrit, english, translit, phonetic])\n",
    "\n",
    "# Convert final list into a DataFrame\n",
    "output_df = pd.DataFrame(final_output, columns=['Sanskrit', 'English', 'Transliteration', 'Phonetics'])\n",
    "\n",
    "# Save the DataFrame to a new CSV file with UTF-8 encoding (for Devanagari support)\n",
    "output_df.to_csv('final_sanskrit_output.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Confirmation message\n",
    "print(\"File generated successfully: final_sanskrit_output.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67642594-e089-47c8-ae63-b68cd9ee28ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Used for working with CSV and tabular data\n",
    "import stanza  # Library for NLP tasks, supports Sanskrit too\n",
    "\n",
    "# Download the Sanskrit language model (only needs to be done once)\n",
    "stanza.download('sa')\n",
    "\n",
    "# Set up the NLP pipeline with tokenization and POS tagging\n",
    "# We set tokenize_no_ssplit=True to keep each input as a single sentence\n",
    "nlp = stanza.Pipeline(lang='sa', processors='tokenize,pos', tokenize_no_ssplit=True)\n",
    "\n",
    "# Read the input CSV which contains Sanskrit words\n",
    "df = pd.read_csv('final_sanskrit_phonetics.csv')\n",
    "\n",
    "# Extract just the Sanskrit column as a list\n",
    "sanskrit_words = df['Sanskrit'].tolist()\n",
    "\n",
    "# Define how to interpret each POS tag syntactically\n",
    "syntactic_map = {\n",
    "    'NOUN': 'Subject/Object',\n",
    "    'VERB': 'Action/State',\n",
    "    'ADJ': 'Describes Noun',\n",
    "    'ADV': 'Describes Verb/Adjective',\n",
    "    'PRON': 'Pronoun Usage',\n",
    "    'ADP': 'Preposition/Postposition',\n",
    "    'DET': 'Determiner',\n",
    "    'NUM': 'Numerical Info',\n",
    "    'AUX': 'Auxiliary Verb',\n",
    "    'PART': 'Particle',\n",
    "    'SCONJ': 'Subordinating Conjunction',\n",
    "    'CCONJ': 'Coordinating Conjunction',\n",
    "    'INTJ': 'Interjection',\n",
    "    'SYM': 'Symbol',\n",
    "    'PUNCT': 'Punctuation'\n",
    "}\n",
    "\n",
    "# Define a dictionary for semantic meanings (manually curated)\n",
    "semantic_map = {\n",
    "    'à¤°à¤¾à¤®à¤ƒ': 'Person - Lord Ram',\n",
    "    'à¤—à¤šà¥à¤›à¤¤à¤¿': 'Verb - To Go',\n",
    "    'à¤«à¤²à¤®à¥': 'Object - Fruit',\n",
    "    'à¤—à¥à¤°à¥à¤ƒ': 'Person - Teacher',\n",
    "    'à¤ªà¥à¤¸à¥à¤¤à¤•à¤®à¥': 'Object - Book'\n",
    "}\n",
    "\n",
    "# Prepare a list to collect results\n",
    "output = []\n",
    "\n",
    "# Loop over each Sanskrit word\n",
    "for word in sanskrit_words:\n",
    "    doc = nlp(word)  # Run the NLP pipeline to get POS tagging\n",
    "    for sent in doc.sentences:  # Iterate over sentence objects (usually one)\n",
    "        for token in sent.words:  # Iterate over word tokens\n",
    "            pos = token.upos  # Get the universal part-of-speech tag\n",
    "             # Map to syntactic function\n",
    "             # Get meaning if available\n",
    "\n",
    "            # Store everything in a dictionary\n",
    "            output.append({\n",
    "                'Sanskrit Word': word,\n",
    "                'POS Tag': pos,\n",
    "                \n",
    "            })\n",
    "\n",
    "# Convert the output list into a DataFrame\n",
    "output_df = pd.DataFrame(output)\n",
    "\n",
    "# Save the final result to a CSV file\n",
    "output_df.to_csv('sanskrit_pos_syntactic_semantic.csv', index=False)\n",
    "\n",
    "# Let the user know the file has been created\n",
    "print(\"CSV Generated Successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
